Transformers: Attention Is All You Need - 2017
- https://arxiv.org/pdf/1706.03762.pdf
- https://arxiv.org/pdf/1803.02155.pdf
- Original Code from Google Brain team : https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py
- https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model
- https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/
- https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&utm_medium=comprehensive-guide-attention-mechanism-deep-learning
- http://jalammar.github.io/illustrated-transformer/
- https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model
- https://github.com/tensorflow/nmt
- https://www.youtube.com/watch?v=53YvP6gdD7U
- https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/
- http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/
- https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a
- https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XeDjgtHhVXM
- https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

Pytorch:
- https://github.com/huggingface/transformers
- http://nlp.seas.harvard.edu/2018/04/03/attention.html
- https://github.com/jadore801120/attention-is-all-you-need-pytorch
- https://github.com/tunz/transformer-pytorch

Tensorflow:
- https://www.tensorflow.org/tutorials/text/transformer (Basics)


Information Extraction with Transformer:
- TENER: Adapting Transformer Encoder for Name Entity Recognition: https://arxiv.org/pdf/1911.04474.pdf
- Chargrid + Transformer : https://www.groundai.com/project/bertgrid-contextualized-embedding-for-2d-document-representation-and-understanding/1
- Bert Grid : https://arxiv.org/pdf/1909.04948.pdf



